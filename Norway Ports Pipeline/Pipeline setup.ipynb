{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f7a671-7113-4b02-ab65-35447488aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First remove some unnecessary columns. this gives some errors in some files. in this code I also got parquet files but I do not pursured those files later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386e0a71-2e8f-4f74-a5d0-2fb8aaaf96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hais_2024-01-01.csv ...\n",
      "Processing hais_2024-01-02.csv ...\n",
      "Processing hais_2024-01-03.csv ...\n",
      "Processing hais_2024-01-04.csv ...\n",
      "Processing hais_2024-01-05.csv ...\n",
      "Processing hais_2024-01-06.csv ...\n",
      "Processing hais_2024-01-07.csv ...\n",
      "Processing hais_2024-01-08.csv ...\n",
      "Processing hais_2024-01-09.csv ...\n",
      "Processing hais_2024-01-10.csv ...\n",
      "Processing hais_2024-01-11.csv ...\n",
      "Processing hais_2024-01-12.csv ...\n",
      "Processing hais_2024-01-13.csv ...\n",
      "Processing hais_2024-01-14.csv ...\n",
      "Processing hais_2024-01-15.csv ...\n",
      "Processing hais_2024-01-16.csv ...\n",
      "Processing hais_2024-01-17.csv ...\n",
      "Processing hais_2024-01-18.csv ...\n",
      "Processing hais_2024-01-19.csv ...\n",
      "Processing hais_2024-01-20.csv ...\n",
      "Processing hais_2024-01-21.csv ...\n",
      "Processing hais_2024-01-22.csv ...\n",
      "Processing hais_2024-01-23.csv ...\n",
      "Processing hais_2024-01-24.csv ...\n",
      "Processing hais_2024-01-25.csv ...\n",
      "Processing hais_2024-01-26.csv ...\n",
      "Processing hais_2024-01-27.csv ...\n",
      "Processing hais_2024-01-28.csv ...\n",
      "Processing hais_2024-01-29.csv ...\n",
      "Processing hais_2024-01-30.csv ...\n",
      "Processing hais_2024-01-31.csv ...\n",
      "Processing hais_2024-02-01.csv ...\n",
      "Processing hais_2024-02-02.csv ...\n",
      "Processing hais_2024-02-03.csv ...\n",
      "Processing hais_2024-02-04.csv ...\n",
      "Processing hais_2024-02-05.csv ...\n",
      "Processing hais_2024-02-06.csv ...\n",
      "Processing hais_2024-02-07.csv ...\n",
      "Processing hais_2024-02-08.csv ...\n",
      "Processing hais_2024-02-09.csv ...\n",
      "Processing hais_2024-02-10.csv ...\n",
      "Processing hais_2024-02-11.csv ...\n",
      "Processing hais_2024-02-12.csv ...\n",
      "Processing hais_2024-02-13.csv ...\n",
      "Processing hais_2024-02-14.csv ...\n",
      "Processing hais_2024-02-15.csv ...\n",
      "Processing hais_2024-02-16.csv ...\n",
      "Processing hais_2024-02-17.csv ...\n",
      "Processing hais_2024-02-18.csv ...\n",
      "Processing hais_2024-02-19.csv ...\n",
      "Processing hais_2024-02-20.csv ...\n",
      "Processing hais_2024-02-21.csv ...\n",
      "Processing hais_2024-02-22.csv ...\n",
      "Processing hais_2024-02-23.csv ...\n",
      "Processing hais_2024-02-24.csv ...\n",
      "Processing hais_2024-02-25.csv ...\n",
      "Processing hais_2024-02-26.csv ...\n",
      "Processing hais_2024-02-27.csv ...\n",
      "Processing hais_2024-02-28.csv ...\n",
      "Processing hais_2024-02-29.csv ...\n",
      "Processing hais_2024-03-01.csv ...\n",
      "Processing hais_2024-03-02.csv ...\n",
      "Processing hais_2024-03-03.csv ...\n",
      "Processing hais_2024-03-04.csv ...\n",
      "Processing hais_2024-03-05.csv ...\n",
      "Processing hais_2024-03-06.csv ...\n",
      "Processing hais_2024-03-07.csv ...\n",
      "Processing hais_2024-03-08.csv ...\n",
      "Processing hais_2024-03-09.csv ...\n",
      "Processing hais_2024-03-10.csv ...\n",
      "Processing hais_2024-03-11.csv ...\n",
      "Processing hais_2024-03-12.csv ...\n",
      "Processing hais_2024-03-13.csv ...\n",
      "Processing hais_2024-03-14.csv ...\n",
      "Processing hais_2024-03-15.csv ...\n",
      "Processing hais_2024-03-16.csv ...\n",
      "Processing hais_2024-03-17.csv ...\n",
      "Processing hais_2024-03-18.csv ...\n",
      "Processing hais_2024-03-19.csv ...\n",
      "Processing hais_2024-03-20.csv ...\n",
      "Processing hais_2024-03-21.csv ...\n",
      "Processing hais_2024-03-22.csv ...\n",
      "Processing hais_2024-03-23.csv ...\n",
      "Processing hais_2024-03-24.csv ...\n",
      "Processing hais_2024-03-25.csv ...\n",
      "Processing hais_2024-03-26.csv ...\n",
      "Processing hais_2024-03-27.csv ...\n",
      "Processing hais_2024-03-28.csv ...\n",
      "Processing hais_2024-03-29.csv ...\n",
      "Processing hais_2024-03-30.csv ...\n",
      "Processing hais_2024-03-31.csv ...\n",
      "Processing hais_2024-04-01.csv ...\n",
      "Processing hais_2024-04-02.csv ...\n",
      "Processing hais_2024-04-03.csv ...\n",
      "Processing hais_2024-04-04.csv ...\n",
      "Processing hais_2024-04-05.csv ...\n",
      "Processing hais_2024-04-06.csv ...\n",
      "Processing hais_2024-04-07.csv ...\n",
      "Processing hais_2024-04-08.csv ...\n",
      "Processing hais_2024-04-09.csv ...\n",
      "Processing hais_2024-04-10.csv ...\n",
      "Processing hais_2024-04-11.csv ...\n",
      "Processing hais_2024-04-12.csv ...\n",
      "Processing hais_2024-04-13.csv ...\n",
      "Processing hais_2024-04-14.csv ...\n",
      "Processing hais_2024-04-15.csv ...\n",
      "Processing hais_2024-04-16.csv ...\n",
      "Processing hais_2024-04-17.csv ...\n",
      "Processing hais_2024-04-18.csv ...\n",
      "Processing hais_2024-04-19.csv ...\n",
      "Processing hais_2024-04-20.csv ...\n",
      "Processing hais_2024-04-21.csv ...\n",
      "Processing hais_2024-04-22.csv ...\n",
      "Processing hais_2024-04-23.csv ...\n",
      "Processing hais_2024-04-24.csv ...\n",
      "Processing hais_2024-04-25.csv ...\n",
      "Processing hais_2024-04-26.csv ...\n",
      "Processing hais_2024-04-27.csv ...\n",
      "Processing hais_2024-04-28.csv ...\n",
      "Processing hais_2024-04-29.csv ...\n",
      "Processing hais_2024-04-30.csv ...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.11 GiB for an array with shape (2, 141658434) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# COMPILE ALL TO ONE FILE\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compiled_csv:\n\u001b[1;32m---> 50\u001b[0m     pd\u001b[38;5;241m.\u001b[39mconcat(compiled_csv, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto_csv(compiled_csv_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compiled_parquet:\n\u001b[0;32m     52\u001b[0m     pd\u001b[38;5;241m.\u001b[39mconcat(compiled_parquet, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto_parquet(compiled_parquet_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.11 GiB for an array with shape (2, 141658434) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIGURATION\n",
    "input_dir = Path(\".\")  # assumes notebook is in same folder as raw CSVs\n",
    "separate_csv_dir = Path(\"output_csv_separate\")\n",
    "separate_parquet_dir = Path(\"output_parquet_separate\")\n",
    "compiled_csv_path = Path(\"output_csv_compiled/compiled.csv\")\n",
    "compiled_parquet_path = Path(\"output_parquet_compiled/compiled.parquet\")\n",
    "\n",
    "# Create output directories\n",
    "separate_csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "separate_parquet_dir.mkdir(parents=True, exist_ok=True)\n",
    "compiled_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "compiled_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"data_source\", \"ais_class\", \"hex_7\", \"hex_14\", \"geometry\"]\n",
    "\n",
    "# Init\n",
    "compiled_csv = []\n",
    "compiled_parquet = []\n",
    "log = []\n",
    "\n",
    "# PROCESS EACH FILE\n",
    "for file in sorted(input_dir.glob(\"*.csv\")):\n",
    "    try:\n",
    "        print(f\"Processing {file.name} ...\")\n",
    "        df = pd.read_csv(file)\n",
    "        rows_before = len(df)\n",
    "\n",
    "        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True, errors='ignore')\n",
    "        rows_after = len(df)\n",
    "\n",
    "        # Save as individual CSV and Parquet\n",
    "        filename = file.stem\n",
    "        df.to_csv(separate_csv_dir / f\"{filename}.csv\", index=False)\n",
    "        df.to_parquet(separate_parquet_dir / f\"{filename}.parquet\", index=False)\n",
    "\n",
    "        # Append to master list\n",
    "        compiled_csv.append(df)\n",
    "        compiled_parquet.append(df)\n",
    "        log.append((file.name, rows_before, rows_after))\n",
    "\n",
    "    except Exception as e:\n",
    "        log.append((file.name, \"ERROR\", str(e)))\n",
    "\n",
    "# COMPILE ALL TO ONE FILE\n",
    "if compiled_csv:\n",
    "    pd.concat(compiled_csv, ignore_index=True).to_csv(compiled_csv_path, index=False)\n",
    "if compiled_parquet:\n",
    "    pd.concat(compiled_parquet, ignore_index=True).to_parquet(compiled_parquet_path, index=False)\n",
    "\n",
    "# LOG OUTPUT\n",
    "print(\"\\n=== Processing Summary ===\")\n",
    "for fname, before, after in log:\n",
    "    print(f\"{fname}: rows before = {before}, rows after = {after}\")\n",
    "print(f\"\\nTotal rows compiled: {sum([x[2] for x in log if isinstance(x[2], int)])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843c3a04-81ef-45b4-8aab-7a9136cb22dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Processing missing files...\n",
      "\n",
      "📄 Reading hais_2024-01-05.csv ...\n",
      "❌ Error processing hais_2024-01-05.csv: Error tokenizing data. C error: EOF inside string starting at row 175732\n",
      "\n",
      "📄 Reading hais_2024-02-13.csv ...\n",
      "❌ Error processing hais_2024-02-13.csv: Error tokenizing data. C error: EOF inside string starting at row 1085892\n",
      "\n",
      "📄 Reading hais_2024-02-14.csv ...\n",
      "❌ Error processing hais_2024-02-14.csv: Error tokenizing data. C error: EOF inside string starting at row 738231\n",
      "\n",
      "📄 Reading hais_2024-02-15.csv ...\n",
      "❌ Error processing hais_2024-02-15.csv: Error tokenizing data. C error: EOF inside string starting at row 1341694\n",
      "\n",
      "📄 Reading hais_2024-02-16.csv ...\n",
      "❌ Error processing hais_2024-02-16.csv: Error tokenizing data. C error: EOF inside string starting at row 116773\n",
      "\n",
      "📄 Reading hais_2024-02-19.csv ...\n",
      "❌ Error processing hais_2024-02-19.csv: Error tokenizing data. C error: EOF inside string starting at row 475130\n",
      "\n",
      "📄 Reading hais_2024-02-22.csv ...\n",
      "❌ Error processing hais_2024-02-22.csv: Error tokenizing data. C error: EOF inside string starting at row 388656\n",
      "\n",
      "📄 Reading hais_2024-04-03.csv ...\n",
      "❌ Error processing hais_2024-04-03.csv: Error tokenizing data. C error: EOF inside string starting at row 508157\n",
      "\n",
      "📄 Reading hais_2024-04-04.csv ...\n",
      "❌ Error processing hais_2024-04-04.csv: Error tokenizing data. C error: EOF inside string starting at row 992934\n",
      "\n",
      "📄 Reading hais_2024-04-05.csv ...\n",
      "❌ Error processing hais_2024-04-05.csv: Error tokenizing data. C error: EOF inside string starting at row 565686\n",
      "\n",
      "📄 Reading hais_2024-04-26.csv ...\n",
      "❌ Error processing hais_2024-04-26.csv: Error tokenizing data. C error: EOF inside string starting at row 1155567\n",
      "\n",
      "📄 Reading hais_2024-04-30.csv ...\n",
      "❌ Error processing hais_2024-04-30.csv: Error tokenizing data. C error: EOF inside string starting at row 574898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- INPUT SETTINGS ---\n",
    "missing_files = [\n",
    "    \"hais_2024-01-05.csv\", \"hais_2024-02-13.csv\", \"hais_2024-02-14.csv\",\n",
    "    \"hais_2024-02-15.csv\", \"hais_2024-02-16.csv\", \"hais_2024-02-19.csv\",\n",
    "    \"hais_2024-02-22.csv\", \"hais_2024-04-03.csv\", \"hais_2024-04-04.csv\",\n",
    "    \"hais_2024-04-05.csv\", \"hais_2024-04-26.csv\", \"hais_2024-04-30.csv\"\n",
    "]\n",
    "\n",
    "columns_to_drop = [\"data_source\", \"ais_class\", \"hex_7\", \"hex_14\", \"geometry\"]\n",
    "\n",
    "# --- OUTPUT FOLDERS ---\n",
    "csv_output_dir = Path(\"output_csv_missing_files\")\n",
    "parquet_output_dir = Path(\"output_parquet_missing_files\")\n",
    "\n",
    "csv_output_dir.mkdir(exist_ok=True)\n",
    "parquet_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# --- PROCESS FILES ---\n",
    "print(\"🔁 Processing missing files...\\n\")\n",
    "\n",
    "for file_name in missing_files:\n",
    "    try:\n",
    "        input_path = Path(file_name)\n",
    "        if not input_path.exists():\n",
    "            print(f\"❌ File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"📄 Reading {file_name} ...\")\n",
    "        df = pd.read_csv(input_path, on_bad_lines='skip')\n",
    "        rows_before = len(df)\n",
    "\n",
    "        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True, errors='ignore')\n",
    "        rows_after = len(df)\n",
    "\n",
    "        # Output paths\n",
    "        csv_output_path = csv_output_dir / file_name\n",
    "        parquet_output_path = parquet_output_dir / file_name.replace(\".csv\", \".parquet\")\n",
    "\n",
    "        # Save filtered files\n",
    "        df.to_csv(csv_output_path, index=False)\n",
    "        df.to_parquet(parquet_output_path, index=False)\n",
    "\n",
    "        print(f\"✅ {file_name} - Rows before: {rows_before}, after: {rows_after}\")\n",
    "        print(f\"   Saved CSV → {csv_output_path}\")\n",
    "        print(f\"   Saved Parquet → {parquet_output_path}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_name}: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "075823f0-6189-443b-83dc-443a2c8e7998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Processing hais_2024-01-05.csv ...\n",
      "❌ Failed to process hais_2024-01-05.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-02-13.csv ...\n",
      "❌ Failed to process hais_2024-02-13.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-02-14.csv ...\n",
      "❌ Failed to process hais_2024-02-14.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-02-15.csv ...\n",
      "❌ Failed to process hais_2024-02-15.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-02-16.csv ...\n",
      "❌ Failed to process hais_2024-02-16.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-02-19.csv ...\n",
      "❌ Failed to process hais_2024-02-19.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-02-22.csv ...\n",
      "❌ Failed to process hais_2024-02-22.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-04-03.csv ...\n",
      "❌ Failed to process hais_2024-04-03.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-04-04.csv ...\n",
      "❌ Failed to process hais_2024-04-04.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-04-05.csv ...\n",
      "❌ Failed to process hais_2024-04-05.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-04-26.csv ...\n",
      "❌ Failed to process hais_2024-04-26.csv | field larger than field limit (131072)\n",
      "\n",
      "🔄 Processing hais_2024-04-30.csv ...\n",
      "❌ Failed to process hais_2024-04-30.csv | field larger than field limit (131072)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Target files\n",
    "missing_files = [\n",
    "    \"hais_2024-01-05.csv\", \"hais_2024-02-13.csv\", \"hais_2024-02-14.csv\",\n",
    "    \"hais_2024-02-15.csv\", \"hais_2024-02-16.csv\", \"hais_2024-02-19.csv\",\n",
    "    \"hais_2024-02-22.csv\", \"hais_2024-04-03.csv\", \"hais_2024-04-04.csv\",\n",
    "    \"hais_2024-04-05.csv\", \"hais_2024-04-26.csv\", \"hais_2024-04-30.csv\"\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "drop_cols = set([\"data_source\", \"ais_class\", \"hex_7\", \"hex_14\", \"geometry\"])\n",
    "\n",
    "# Output dirs\n",
    "base_dir = Path(\".\")\n",
    "csv_output_dir = base_dir / \"output_csv_missing_files_safe_v2\"\n",
    "parquet_output_dir = base_dir / \"output_parquet_missing_files_safe_v2\"\n",
    "csv_output_dir.mkdir(exist_ok=True)\n",
    "parquet_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Process each file line-by-line\n",
    "for file in missing_files:\n",
    "    raw_path = base_dir / file\n",
    "    csv_out_path = csv_output_dir / file\n",
    "    parquet_out_path = parquet_output_dir / file.replace(\".csv\", \".parquet\")\n",
    "\n",
    "    print(f\"🔄 Processing {file} ...\")\n",
    "    try:\n",
    "        with open(raw_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f_in, open(csv_out_path, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
    "            reader = csv.DictReader(f_in)\n",
    "            # Remove unwanted columns from header\n",
    "            clean_fields = [field for field in reader.fieldnames if field not in drop_cols]\n",
    "\n",
    "            writer = csv.DictWriter(f_out, fieldnames=clean_fields)\n",
    "            writer.writeheader()\n",
    "\n",
    "            total = 0\n",
    "            kept = 0\n",
    "            for row in reader:\n",
    "                total += 1\n",
    "                try:\n",
    "                    clean_row = {k: row[k] for k in clean_fields}\n",
    "                    writer.writerow(clean_row)\n",
    "                    kept += 1\n",
    "                except Exception:\n",
    "                    # Skip broken rows\n",
    "                    continue\n",
    "\n",
    "        # Save to Parquet\n",
    "        print(f\"📦 Reading filtered {file} for Parquet export...\")\n",
    "        df = pd.read_csv(csv_out_path)\n",
    "        df.to_parquet(parquet_out_path, index=False)\n",
    "\n",
    "        print(f\"✅ Done: {file} | Rows Kept: {kept} / {total}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to process {file} | {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7410828a-8c57-4f84-954e-cd5e5f8bc3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Cleaning hais_2024-01-05.csv ...\n",
      "❌ Failed on hais_2024-01-05.csv with error: field larger than field limit (67108864)\n",
      "\n",
      "🔧 Cleaning hais_2024-02-13.csv ...\n",
      "✅ Cleaned CSV saved (1085891 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-02-13.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-02-14.csv ...\n",
      "❌ Failed on hais_2024-02-14.csv with error: field larger than field limit (67108864)\n",
      "\n",
      "🔧 Cleaning hais_2024-02-15.csv ...\n",
      "✅ Cleaned CSV saved (1341693 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-02-15.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-02-16.csv ...\n",
      "❌ Failed on hais_2024-02-16.csv with error: field larger than field limit (67108864)\n",
      "\n",
      "🔧 Cleaning hais_2024-02-19.csv ...\n",
      "❌ Failed on hais_2024-02-19.csv with error: field larger than field limit (67108864)\n",
      "\n",
      "🔧 Cleaning hais_2024-02-22.csv ...\n",
      "❌ Failed on hais_2024-02-22.csv with error: field larger than field limit (67108864)\n",
      "\n",
      "🔧 Cleaning hais_2024-04-03.csv ...\n",
      "❌ Failed on hais_2024-04-03.csv with error: field larger than field limit (67108864)\n",
      "\n",
      "🔧 Cleaning hais_2024-04-04.csv ...\n",
      "✅ Cleaned CSV saved (992933 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-04-04.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-04-05.csv ...\n",
      "✅ Cleaned CSV saved (565685 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-04-05.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-04-26.csv ...\n",
      "✅ Cleaned CSV saved (1155566 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-04-26.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-04-30.csv ...\n",
      "❌ Failed on hais_2024-04-30.csv with error: field larger than field limit (67108864)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Folders\n",
    "raw_data_folder = os.getcwd()\n",
    "cleaned_csv_folder = \"final_cleaned_csv\"\n",
    "cleaned_parquet_folder = \"final_cleaned_parquet\"\n",
    "os.makedirs(cleaned_csv_folder, exist_ok=True)\n",
    "os.makedirs(cleaned_parquet_folder, exist_ok=True)\n",
    "\n",
    "# Bad files list\n",
    "bad_files = [\n",
    "    \"hais_2024-01-05.csv\", \"hais_2024-02-13.csv\", \"hais_2024-02-14.csv\",\n",
    "    \"hais_2024-02-15.csv\", \"hais_2024-02-16.csv\", \"hais_2024-02-19.csv\",\n",
    "    \"hais_2024-02-22.csv\", \"hais_2024-04-03.csv\", \"hais_2024-04-04.csv\",\n",
    "    \"hais_2024-04-05.csv\", \"hais_2024-04-26.csv\", \"hais_2024-04-30.csv\"\n",
    "]\n",
    "\n",
    "columns_to_drop = ['data_source', 'ais_class', 'hex_7', 'hex_14', 'geometry']\n",
    "\n",
    "# Process each bad file\n",
    "for file_name in bad_files:\n",
    "    input_path = os.path.join(raw_data_folder, file_name)\n",
    "    cleaned_csv_path = os.path.join(cleaned_csv_folder, file_name)\n",
    "    cleaned_parquet_path = os.path.join(cleaned_parquet_folder, file_name.replace(\".csv\", \".parquet\"))\n",
    "\n",
    "    print(f\"🔧 Cleaning {file_name} ...\")\n",
    "\n",
    "    try:\n",
    "        # Open and clean manually\n",
    "        with open(input_path, 'r', encoding='utf-8', errors='replace') as infile, \\\n",
    "             open(cleaned_csv_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile)\n",
    "\n",
    "            # Try to read/write header\n",
    "            try:\n",
    "                headers = next(reader)\n",
    "                writer.writerow(headers)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Couldn't read header for {file_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Process lines safely\n",
    "            good_lines = 0\n",
    "            bad_lines = 0\n",
    "            for line in reader:\n",
    "                try:\n",
    "                    if len(line) == len(headers):  # Basic line length check\n",
    "                        writer.writerow(line)\n",
    "                        good_lines += 1\n",
    "                    else:\n",
    "                        bad_lines += 1\n",
    "                except:\n",
    "                    bad_lines += 1\n",
    "\n",
    "        print(f\"✅ Cleaned CSV saved ({good_lines} good lines, {bad_lines} bad lines)\")\n",
    "\n",
    "        # Now load with pandas and save parquet\n",
    "        df = pd.read_csv(cleaned_csv_path, low_memory=False)\n",
    "\n",
    "        # Drop specified columns if they exist\n",
    "        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True, errors='ignore')\n",
    "\n",
    "        # Save parquet\n",
    "        df.to_parquet(cleaned_parquet_path, index=False)\n",
    "\n",
    "        print(f\"✅ Parquet saved: {file_name}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed on {file_name} with error: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4cf4361-0efe-4829-8583-dc532f749145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Cleaning hais_2024-01-05.csv ...\n",
      "✅ Cleaned CSV saved (175731 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-01-05.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-02-13.csv ...\n",
      "✅ Cleaned CSV saved (1085891 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-02-13.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-02-14.csv ...\n",
      "✅ Cleaned CSV saved (738230 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-02-14.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-02-15.csv ...\n",
      "✅ Cleaned CSV saved (1341693 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-02-15.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-02-16.csv ...\n",
      "✅ Cleaned CSV saved (116772 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-02-16.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-02-19.csv ...\n",
      "✅ Cleaned CSV saved (475129 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-02-19.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-02-22.csv ...\n",
      "✅ Cleaned CSV saved (388655 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-02-22.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-04-03.csv ...\n",
      "✅ Cleaned CSV saved (508156 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-04-03.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-04-04.csv ...\n",
      "✅ Cleaned CSV saved (992933 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-04-04.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-04-05.csv ...\n",
      "✅ Cleaned CSV saved (565685 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-04-05.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-04-26.csv ...\n",
      "✅ Cleaned CSV saved (1155566 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-04-26.csv\n",
      "\n",
      "🔧 Cleaning hais_2024-04-30.csv ...\n",
      "✅ Cleaned CSV saved (574897 good lines, 1 bad lines)\n",
      "✅ Parquet saved: hais_2024-04-30.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Set safe maximum CSV field size\n",
    "max_int = sys.maxsize\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(max_int)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        max_int = int(max_int / 10)\n",
    "\n",
    "# Define folder paths\n",
    "raw_data_folder = os.getcwd()\n",
    "csv_output_folder = os.path.join(raw_data_folder, \"cleaned_remaining_csv\")\n",
    "parquet_output_folder = os.path.join(raw_data_folder, \"cleaned_remaining_parquet\")\n",
    "\n",
    "# Ensure output folders exist\n",
    "os.makedirs(csv_output_folder, exist_ok=True)\n",
    "os.makedirs(parquet_output_folder, exist_ok=True)\n",
    "\n",
    "# Files that previously failed\n",
    "remaining_files = [\n",
    "    \"hais_2024-01-05.csv\", \"hais_2024-02-13.csv\", \"hais_2024-02-14.csv\",\n",
    "    \"hais_2024-02-15.csv\", \"hais_2024-02-16.csv\", \"hais_2024-02-19.csv\",\n",
    "    \"hais_2024-02-22.csv\", \"hais_2024-04-03.csv\", \"hais_2024-04-04.csv\",\n",
    "    \"hais_2024-04-05.csv\", \"hais_2024-04-26.csv\", \"hais_2024-04-30.csv\"\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = ['data_source', 'ais_class', 'hex_7', 'hex_14', 'geometry']\n",
    "\n",
    "def clean_and_process_file(file_name):\n",
    "    file_path = os.path.join(raw_data_folder, file_name)\n",
    "    output_csv_path = os.path.join(csv_output_folder, file_name)\n",
    "    output_parquet_path = os.path.join(parquet_output_folder, file_name.replace(\".csv\", \".parquet\"))\n",
    "\n",
    "    print(f\"\\n🔧 Cleaning {file_name} ...\")\n",
    "\n",
    "    good_lines = []\n",
    "    header = None\n",
    "    bad_lines = 0\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='replace') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            header = next(reader)\n",
    "            for row_num, row in enumerate(reader, start=2):\n",
    "                if len(row) == len(header):\n",
    "                    good_lines.append(row)\n",
    "                else:\n",
    "                    bad_lines += 1\n",
    "\n",
    "        if not good_lines:\n",
    "            print(f\"❌ No good lines found in {file_name}\")\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(good_lines, columns=header)\n",
    "\n",
    "        # Drop columns if they exist\n",
    "        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True, errors='ignore')\n",
    "\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        df.to_parquet(output_parquet_path, index=False)\n",
    "\n",
    "        print(f\"✅ Cleaned CSV saved ({len(good_lines)} good lines, {bad_lines} bad lines)\")\n",
    "        print(f\"✅ Parquet saved: {file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed on {file_name} with error: {e}\")\n",
    "\n",
    "# Run cleaning\n",
    "for file in remaining_files:\n",
    "    clean_and_process_file(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f08a4420-3730-4b4f-a764-2018dc0a034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary created: row_comparison_summary.csv\n",
      "                    File  Raw Rows  Processed CSV Rows  \\\n",
      "0    hais_2024-01-01.csv   1264740             1264740   \n",
      "1    hais_2024-01-02.csv   1168608             1168608   \n",
      "2    hais_2024-01-03.csv   1213077             1213077   \n",
      "3    hais_2024-01-04.csv   1294664             1294664   \n",
      "4    hais_2024-01-05.csv   1340991              175778   \n",
      "..                   ...       ...                 ...   \n",
      "116  hais_2024-04-26.csv   1196245             1155577   \n",
      "117  hais_2024-04-27.csv   1290112             1290112   \n",
      "118  hais_2024-04-28.csv   1366811             1366811   \n",
      "119  hais_2024-04-29.csv   1438768             1438768   \n",
      "120  hais_2024-04-30.csv   1358923              574995   \n",
      "\n",
      "     Processed Parquet Rows  Removed Rows  \n",
      "0                   1264740             0  \n",
      "1                   1168608             0  \n",
      "2                   1213077             0  \n",
      "3                   1294664             0  \n",
      "4                    175731       1165213  \n",
      "..                      ...           ...  \n",
      "116                 1155566         40668  \n",
      "117                 1290112             0  \n",
      "118                 1366811             0  \n",
      "119                 1438768             0  \n",
      "120                  574897        783928  \n",
      "\n",
      "[121 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Folders\n",
    "raw_folder = os.getcwd()\n",
    "csv_processed_folder = os.path.join(raw_folder, \"output_csv_separate\")\n",
    "parquet_processed_folder = os.path.join(raw_folder, \"output_parquet_separate\")\n",
    "\n",
    "# Only include .csv files from raw\n",
    "raw_files = [f for f in os.listdir(raw_folder) if f.endswith(\".csv\") and f.startswith(\"hais_\")]\n",
    "\n",
    "# Summary list\n",
    "summary = []\n",
    "\n",
    "def count_csv_rows(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        return sum(1 for line in f) - 1  # exclude header\n",
    "\n",
    "def count_parquet_rows(filepath):\n",
    "    try:\n",
    "        df = pd.read_parquet(filepath)\n",
    "        return len(df)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for filename in sorted(raw_files):\n",
    "    raw_path = os.path.join(raw_folder, filename)\n",
    "    csv_proc_path = os.path.join(csv_processed_folder, filename)\n",
    "    parquet_proc_path = os.path.join(parquet_processed_folder, filename.replace(\".csv\", \".parquet\"))\n",
    "\n",
    "    raw_rows = count_csv_rows(raw_path)\n",
    "    csv_rows = count_csv_rows(csv_proc_path) if os.path.exists(csv_proc_path) else None\n",
    "    parquet_rows = count_parquet_rows(parquet_proc_path) if os.path.exists(parquet_proc_path) else None\n",
    "\n",
    "    summary.append({\n",
    "        \"File\": filename,\n",
    "        \"Raw Rows\": raw_rows,\n",
    "        \"Processed CSV Rows\": csv_rows,\n",
    "        \"Processed Parquet Rows\": parquet_rows,\n",
    "        \"Removed Rows\": raw_rows - csv_rows if csv_rows is not None else None\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for display/export\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_csv(\"row_comparison_summary.csv\", index=False)\n",
    "\n",
    "print(\"✅ Summary created: row_comparison_summary.csv\")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4227d2fb-b564-4a4e-9c59-bbda90e4fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Processing hais_2024-01-05.csv ...\n",
      "✅ Done: hais_2024-01-05.csv | Rows: 175732 → 175732 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-02-13.csv ...\n",
      "✅ Done: hais_2024-02-13.csv | Rows: 1085892 → 1085892 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-02-14.csv ...\n",
      "✅ Done: hais_2024-02-14.csv | Rows: 738231 → 738231 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-02-15.csv ...\n",
      "✅ Done: hais_2024-02-15.csv | Rows: 1341694 → 1341694 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-02-16.csv ...\n",
      "✅ Done: hais_2024-02-16.csv | Rows: 116773 → 116773 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-02-19.csv ...\n",
      "✅ Done: hais_2024-02-19.csv | Rows: 475130 → 475130 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-02-22.csv ...\n",
      "✅ Done: hais_2024-02-22.csv | Rows: 388656 → 388656 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-04-03.csv ...\n",
      "✅ Done: hais_2024-04-03.csv | Rows: 508157 → 508157 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-04-04.csv ...\n",
      "✅ Done: hais_2024-04-04.csv | Rows: 992934 → 992934 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-04-05.csv ...\n",
      "✅ Done: hais_2024-04-05.csv | Rows: 565686 → 565686 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-04-26.csv ...\n",
      "✅ Done: hais_2024-04-26.csv | Rows: 1155567 → 1155567 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "🔧 Processing hais_2024-04-30.csv ...\n",
      "✅ Done: hais_2024-04-30.csv | Rows: 574898 → 574898 | Fixed: 1 | Dropped: ['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "\n",
      "📊 SUMMARY:\n",
      "File                       Original   Cleaned     Fixed             Dropped\n",
      "hais_2024-01-05.csv         175,732   175,732         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-02-13.csv       1,085,892 1,085,892         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-02-14.csv         738,231   738,231         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-02-15.csv       1,341,694 1,341,694         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-02-16.csv         116,773   116,773         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-02-19.csv         475,130   475,130         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-02-22.csv         388,656   388,656         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-04-03.csv         508,157   508,157         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-04-04.csv         992,934   992,934         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-04-05.csv         565,686   565,686         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-04-26.csv       1,155,567 1,155,567         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "hais_2024-04-30.csv         574,898   574,898         1['data_source', 'hex_14', 'ais_class', 'geometry', 'hex_7']\n",
      "\n",
      "🧪 EXAMPLE FIXED ROWS (max 3):\n",
      "01: ['2024-01-05T10:08:44.000Z', '257036620', '5.221785', '60.269737', '0'] ... [len=20]\n",
      "02: ['2024-02-13T13:26:05.000Z', '257036620', '5.221825', '60.269733', '0'] ... [len=20]\n",
      "03: ['2024-02-14T08:05:19.000Z', '257036620', '5.2218', '60.269733', '0'] ... [len=20]\n",
      "04: ['2024-02-15T11:03:04.000Z', '257036620', '5.20066', '60.236774', '0'] ... [len=20]\n",
      "05: ['2024-02-16T09:10:22.000Z', '257036620', '5.221805', '60.269733', '0'] ... [len=20]\n",
      "06: ['2024-02-19T10:54:09.000Z', '257036620', '5.17742', '60.27357', '0'] ... [len=20]\n",
      "07: ['2024-02-22T10:48:11.000Z', '257036620', '5.22184', '60.269726', '0'] ... [len=20]\n",
      "08: ['2024-04-03T11:21:22.000Z', '257036620', '5.2218', '60.26973', '0'] ... [len=20]\n",
      "09: ['2024-04-04T08:12:31.000Z', '257036620', '5.2217965', '60.26974', '0'] ... [len=20]\n",
      "10: ['2024-04-05T07:42:10.000Z', '257036620', '5.065575', '60.419155', '0'] ... [len=20]\n",
      "11: ['2024-04-26T08:02:03.000Z', '257036620', '5.221803', '60.26973', '0'] ... [len=20]\n",
      "12: ['2024-04-30T12:09:53.000Z', '257036620', '5.221812', '60.269703', '0'] ... [len=20]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Fix field limit\n",
    "csv.field_size_limit(2**31 - 1)\n",
    "\n",
    "# Files to reprocess\n",
    "problematic_files = [\n",
    "    \"hais_2024-01-05.csv\", \"hais_2024-02-13.csv\", \"hais_2024-02-14.csv\",\n",
    "    \"hais_2024-02-15.csv\", \"hais_2024-02-16.csv\", \"hais_2024-02-19.csv\",\n",
    "    \"hais_2024-02-22.csv\", \"hais_2024-04-03.csv\", \"hais_2024-04-04.csv\",\n",
    "    \"hais_2024-04-05.csv\", \"hais_2024-04-26.csv\", \"hais_2024-04-30.csv\"\n",
    "]\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = {'data_source', 'ais_class', 'hex_7', 'hex_14', 'geometry'}\n",
    "\n",
    "# Output folders\n",
    "raw_folder = Path.cwd()\n",
    "csv_out = raw_folder / \"output_csv_cleaned_final\"\n",
    "parquet_out = raw_folder / \"output_parquet_cleaned_final\"\n",
    "csv_out.mkdir(exist_ok=True)\n",
    "parquet_out.mkdir(exist_ok=True)\n",
    "\n",
    "summary = []\n",
    "preview_bad_rows = []\n",
    "\n",
    "def is_valid_row(row, expected_len):\n",
    "    return len(row) == expected_len\n",
    "\n",
    "def fix_row(row, expected_len):\n",
    "    if len(row) > expected_len:\n",
    "        return row[:expected_len - 1] + [' '.join(row[expected_len - 1:])]\n",
    "    elif len(row) < expected_len:\n",
    "        return row + [''] * (expected_len - len(row))\n",
    "    return row\n",
    "\n",
    "for fname in problematic_files:\n",
    "    print(f\"🔧 Processing {fname} ...\")\n",
    "    fpath = raw_folder / fname\n",
    "    if not fpath.exists():\n",
    "        print(f\"❌ Missing: {fname}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\", errors=\"replace\", newline=\"\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            headers = next(reader)\n",
    "            expected_len = len(headers)\n",
    "            cleaned_rows = [headers]\n",
    "            total = 0\n",
    "            fixed = 0\n",
    "            previewed = 0\n",
    "\n",
    "            for row in reader:\n",
    "                total += 1\n",
    "                if is_valid_row(row, expected_len):\n",
    "                    cleaned_rows.append(row)\n",
    "                else:\n",
    "                    fixed_row = fix_row(row, expected_len)\n",
    "                    cleaned_rows.append(fixed_row)\n",
    "                    fixed += 1\n",
    "                    if previewed < 3:\n",
    "                        preview_bad_rows.append(fixed_row)\n",
    "                        previewed += 1\n",
    "\n",
    "        # Convert to DataFrame and drop unwanted columns\n",
    "        df = pd.DataFrame(cleaned_rows[1:], columns=cleaned_rows[0])\n",
    "        dropped = [col for col in columns_to_drop if col in df.columns]\n",
    "        df.drop(columns=dropped, inplace=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_path = csv_out / fname\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        # Save to Parquet\n",
    "        parquet_path = parquet_out / fname.replace(\".csv\", \".parquet\")\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "        summary.append({\n",
    "            \"File\": fname,\n",
    "            \"Original Rows\": total,\n",
    "            \"Cleaned Rows\": len(df),\n",
    "            \"Fixed Rows\": fixed,\n",
    "            \"Dropped Cols\": dropped\n",
    "        })\n",
    "\n",
    "        print(f\"✅ Done: {fname} | Rows: {total} → {len(df)} | Fixed: {fixed} | Dropped: {dropped}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {fname} | {e}\")\n",
    "\n",
    "# Final concise summary\n",
    "print(\"\\n📊 SUMMARY:\")\n",
    "print(f\"{'File':<25}{'Original':>10}{'Cleaned':>10}{'Fixed':>10}{'Dropped':>20}\")\n",
    "for s in summary:\n",
    "    print(f\"{s['File']:<25}{s['Original Rows']:>10,}{s['Cleaned Rows']:>10,}{s['Fixed Rows']:>10,}{str(s['Dropped Cols']):>20}\")\n",
    "\n",
    "# Show sample of fixed bad rows (limit to 3)\n",
    "print(\"\\n🧪 EXAMPLE FIXED ROWS (max 3):\")\n",
    "for i, row in enumerate(preview_bad_rows):\n",
    "    print(f\"{i+1:02d}: {row[:5]} ... [len={len(row)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a642a8-b239-4c9b-be02-4113511eb963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Comparing row counts...\n",
      "\n",
      "📊 Row Comparison Summary (top 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Raw Rows</th>\n",
       "      <th>Filtered CSV Rows</th>\n",
       "      <th>Filtered Parquet Rows</th>\n",
       "      <th>CSV Loss</th>\n",
       "      <th>Parquet Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hais_2024-01-01.csv</td>\n",
       "      <td>1264740</td>\n",
       "      <td>1264740</td>\n",
       "      <td>1264740</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hais_2024-01-02.csv</td>\n",
       "      <td>1168608</td>\n",
       "      <td>1168608</td>\n",
       "      <td>1168608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hais_2024-01-03.csv</td>\n",
       "      <td>1213077</td>\n",
       "      <td>1213077</td>\n",
       "      <td>1213077</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hais_2024-01-04.csv</td>\n",
       "      <td>1294664</td>\n",
       "      <td>1294664</td>\n",
       "      <td>1294664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hais_2024-01-05.csv</td>\n",
       "      <td>1340991</td>\n",
       "      <td>1340992</td>\n",
       "      <td>175732</td>\n",
       "      <td>-1</td>\n",
       "      <td>1165259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hais_2024-01-06.csv</td>\n",
       "      <td>1221781</td>\n",
       "      <td>1221781</td>\n",
       "      <td>1221781</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hais_2024-01-07.csv</td>\n",
       "      <td>1123060</td>\n",
       "      <td>1123060</td>\n",
       "      <td>1123060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hais_2024-01-08.csv</td>\n",
       "      <td>1208519</td>\n",
       "      <td>1208519</td>\n",
       "      <td>1208519</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hais_2024-01-09.csv</td>\n",
       "      <td>1198969</td>\n",
       "      <td>1198969</td>\n",
       "      <td>1198969</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hais_2024-01-10.csv</td>\n",
       "      <td>1238576</td>\n",
       "      <td>1238576</td>\n",
       "      <td>1238576</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  File  Raw Rows  Filtered CSV Rows  Filtered Parquet Rows  \\\n",
       "0  hais_2024-01-01.csv   1264740            1264740                1264740   \n",
       "1  hais_2024-01-02.csv   1168608            1168608                1168608   \n",
       "2  hais_2024-01-03.csv   1213077            1213077                1213077   \n",
       "3  hais_2024-01-04.csv   1294664            1294664                1294664   \n",
       "4  hais_2024-01-05.csv   1340991            1340992                 175732   \n",
       "5  hais_2024-01-06.csv   1221781            1221781                1221781   \n",
       "6  hais_2024-01-07.csv   1123060            1123060                1123060   \n",
       "7  hais_2024-01-08.csv   1208519            1208519                1208519   \n",
       "8  hais_2024-01-09.csv   1198969            1198969                1198969   \n",
       "9  hais_2024-01-10.csv   1238576            1238576                1238576   \n",
       "\n",
       "   CSV Loss  Parquet Loss  \n",
       "0         0             0  \n",
       "1         0             0  \n",
       "2         0             0  \n",
       "3         0             0  \n",
       "4        -1       1165259  \n",
       "5         0             0  \n",
       "6         0             0  \n",
       "7         0             0  \n",
       "8         0             0  \n",
       "9         0             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Summary saved to: C:\\Users\\herox\\Documents\\Thesis Work\\Norway Ports Data Filtered\\row_comparison_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "base_path = Path.cwd()\n",
    "raw_files = list(base_path.glob(\"hais_*.csv\"))\n",
    "filtered_csv_path = base_path / \"output_csv_separate\"\n",
    "filtered_parquet_path = base_path / \"output_parquet_separate\"\n",
    "\n",
    "# Summary data\n",
    "summary = []\n",
    "\n",
    "print(\"🔍 Comparing row counts...\")\n",
    "\n",
    "for raw_file in raw_files:\n",
    "    filename = raw_file.name\n",
    "    raw_count = 0\n",
    "    csv_count = None\n",
    "    parquet_count = None\n",
    "\n",
    "    try:\n",
    "        raw_count = sum(1 for _ in open(raw_file, encoding=\"utf-8\", errors=\"replace\")) - 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading raw file: {filename} | {e}\")\n",
    "\n",
    "    filtered_csv_file = filtered_csv_path / filename\n",
    "    filtered_parquet_file = filtered_parquet_path / filename.replace(\".csv\", \".parquet\")\n",
    "\n",
    "    if filtered_csv_file.exists():\n",
    "        try:\n",
    "            csv_count = sum(1 for _ in open(filtered_csv_file, encoding=\"utf-8\", errors=\"replace\")) - 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading filtered CSV: {filtered_csv_file.name} | {e}\")\n",
    "\n",
    "    if filtered_parquet_file.exists():\n",
    "        try:\n",
    "            df_parquet = pd.read_parquet(filtered_parquet_file)\n",
    "            parquet_count = len(df_parquet)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading Parquet: {filtered_parquet_file.name} | {e}\")\n",
    "\n",
    "    summary.append({\n",
    "        \"File\": filename,\n",
    "        \"Raw Rows\": raw_count,\n",
    "        \"Filtered CSV Rows\": csv_count if csv_count is not None else \"Missing\",\n",
    "        \"Filtered Parquet Rows\": parquet_count if parquet_count is not None else \"Missing\",\n",
    "        \"CSV Loss\": raw_count - csv_count if csv_count is not None else \"N/A\",\n",
    "        \"Parquet Loss\": raw_count - parquet_count if parquet_count is not None else \"N/A\"\n",
    "    })\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Save summary\n",
    "summary_file = base_path / \"row_comparison_summary.csv\"\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "# Display small preview\n",
    "print(\"\\n📊 Row Comparison Summary (top 10):\")\n",
    "display(summary_df.head(10))\n",
    "\n",
    "print(f\"\\n✅ Summary saved to: {summary_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b91349-c2c2-4ddf-b8a3-b32ffb6ea1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
