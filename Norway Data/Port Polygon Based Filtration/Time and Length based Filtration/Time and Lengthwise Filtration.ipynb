{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9acca48d-682e-4de8-a86f-881908ab8b54",
   "metadata": {},
   "source": [
    "## Remove vessels below 65 meters and keep one record for 2 minutes timeframe \n",
    "Since below 65 meters length vessel will not be a true cargo or tanker vessel so we can drop those vessels. Also, there are too many records for each unique vessel between 1 minute timeframe based on timestamps provided in data. So we can also reduce the data size by reducing the record for each unique vessel to keep 1 record per two minutes timeframe based on timestamps in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c492da1-6cfb-4953-a045-44c55e0a3533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Bergen Terminal.csv ...\n",
      " • Unique vessels before: 129\n",
      " • Unique vessels after:  129\n",
      " • Rows written: 428016 to D:\\Thesis Work MLS\\Norway Data Filtered\\Port_Split_Result\\Cleaned\\Bergen Terminal.csv\n",
      "\n",
      "Processing Drammen Port.csv ...\n",
      " • Unique vessels before: 77\n",
      " • Unique vessels after:  77\n",
      " • Rows written: 122155 to D:\\Thesis Work MLS\\Norway Data Filtered\\Port_Split_Result\\Cleaned\\Drammen Port.csv\n",
      "\n",
      "Processing Kristiansand Terminal.csv ...\n",
      " • Unique vessels before: 81\n",
      " • Unique vessels after:  81\n",
      " • Rows written: 119692 to D:\\Thesis Work MLS\\Norway Data Filtered\\Port_Split_Result\\Cleaned\\Kristiansand Terminal.csv\n",
      "\n",
      "Processing Oslo Port Area.csv ...\n",
      " • Unique vessels before: 152\n",
      " • Unique vessels after:  152\n",
      " • Rows written: 177779 to D:\\Thesis Work MLS\\Norway Data Filtered\\Port_Split_Result\\Cleaned\\Oslo Port Area.csv\n",
      "\n",
      "Processing Stavanger Westport Terminal.csv ...\n",
      " • Unique vessels before: 138\n",
      " • Unique vessels after:  138\n",
      " • Rows written: 336406 to D:\\Thesis Work MLS\\Norway Data Filtered\\Port_Split_Result\\Cleaned\\Stavanger Westport Terminal.csv\n",
      "\n",
      "ALL PORTS CLEANED AND SAVED.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# --- Folders ---\n",
    "input_folder = r'D:\\Thesis Work MLS\\Norway Data Filtered\\Port_Split_Result'\n",
    "output_folder = os.path.join(input_folder, 'Cleaned')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "files = [f for f in os.listdir(input_folder) if f.lower().endswith('.csv')]\n",
    "\n",
    "for fname in files:\n",
    "    print(f\"\\nProcessing {fname} ...\")\n",
    "    in_path = os.path.join(input_folder, fname)\n",
    "    out_path = os.path.join(output_folder, fname)\n",
    "\n",
    "    # Read file as string, then convert date and length columns explicitly\n",
    "    df = pd.read_csv(in_path, dtype=str)\n",
    "    # Drop rows with missing key fields\n",
    "    df = df.dropna(subset=['mmsi', 'date_time_utc', 'length'])\n",
    "\n",
    "    # Convert to correct types\n",
    "    df['length'] = pd.to_numeric(df['length'], errors='coerce')\n",
    "    df = df.dropna(subset=['length'])\n",
    "    df = df[df['length'] >= 65]   # Only vessels >=65 meters\n",
    "\n",
    "    # Make sure timestamp parsing is strict!\n",
    "    df['date_time_utc'] = pd.to_datetime(df['date_time_utc'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce')\n",
    "    df = df.dropna(subset=['date_time_utc'])\n",
    "\n",
    "    before_vessels = df['mmsi'].nunique()\n",
    "\n",
    "    # For each vessel, keep one row every 2 min based on timestamp\n",
    "    df = df.sort_values(['mmsi', 'date_time_utc'])\n",
    "    keep_rows = []\n",
    "    for mmsi, group in df.groupby('mmsi'):\n",
    "        # For each group, keep first row, then only keep rows >= 2min apart from last kept\n",
    "        last_time = pd.Timestamp.min\n",
    "        for idx, row in group.iterrows():\n",
    "            if row['date_time_utc'] >= last_time + timedelta(minutes=2):\n",
    "                keep_rows.append(idx)\n",
    "                last_time = row['date_time_utc']\n",
    "\n",
    "    filtered_df = df.loc[keep_rows]\n",
    "    after_vessels = filtered_df['mmsi'].nunique()\n",
    "\n",
    "    # Save result (keep all columns as original)\n",
    "    filtered_df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\" • Unique vessels before: {before_vessels}\")\n",
    "    print(f\" • Unique vessels after:  {after_vessels}\")\n",
    "    print(f\" • Rows written: {len(filtered_df)} to {out_path}\")\n",
    "\n",
    "print(\"\\nALL PORTS CLEANED AND SAVED.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72ae82-b899-4681-b65b-6227e2abd664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
